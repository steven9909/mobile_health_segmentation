{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "batch_size should be a positive integer value, but got batch_size=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/stevenhyun/Desktop/mobile_health_segmentation/segmentation/data/visualize.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stevenhyun/Desktop/mobile_health_segmentation/segmentation/data/visualize.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m image, mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_files[idx]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stevenhyun/Desktop/mobile_health_segmentation/segmentation/data/visualize.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m dataset \u001b[39m=\u001b[39m TempDataset(image_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../original\u001b[39m\u001b[39m\"\u001b[39m, mask_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../annotated\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stevenhyun/Desktop/mobile_health_segmentation/segmentation/data/visualize.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(dataset))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stevenhyun/Desktop/mobile_health_segmentation/segmentation/data/visualize.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m image, mask, name \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stevenhyun/Desktop/mobile_health_segmentation/segmentation/data/visualize.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     unique \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munique(mask)\n",
      "File \u001b[0;32m~/Desktop/mobile_health_segmentation/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:355\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    351\u001b[0m             sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m batch_sampler \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     \u001b[39m# auto_collation without custom batch_sampler\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     batch_sampler \u001b[39m=\u001b[39m BatchSampler(sampler, batch_size, drop_last)\n\u001b[1;32m    357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m    358\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_last \u001b[39m=\u001b[39m drop_last\n",
      "File \u001b[0;32m~/Desktop/mobile_health_segmentation/venv/lib/python3.11/site-packages/torch/utils/data/sampler.py:263\u001b[0m, in \u001b[0;36mBatchSampler.__init__\u001b[0;34m(self, sampler, batch_size, drop_last)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, sampler: Union[Sampler[\u001b[39mint\u001b[39m], Iterable[\u001b[39mint\u001b[39m]], batch_size: \u001b[39mint\u001b[39m, drop_last: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[39m# Since collections.abc.Iterable does not check for `__getitem__`, which\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# is one way for an object to be an iterable, we don't do an `isinstance`\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39m# check here.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_size, \u001b[39mbool\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    262\u001b[0m             batch_size \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 263\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch_size should be a positive integer value, but got batch_size=\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(drop_last, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    265\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdrop_last should be a boolean value, but got drop_last=\u001b[39m\u001b[39m{\u001b[39;00mdrop_last\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: batch_size should be a positive integer value, but got batch_size=0"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class TempDataset(Dataset):\n",
    "    def __init__(self, image_path, mask_path):\n",
    "        self.image_files = glob.glob(image_path + \"/*.jpg\")\n",
    "        self.mask_files = glob.glob(mask_path + \"/*.png\")\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = np.asarray(Image.open(self.image_files[idx]).convert(\"RGB\"))\n",
    "        mask = np.asarray(Image.open(self.mask_files[idx]).convert(\"L\"))\n",
    "        image = self.transform(image)\n",
    "        mask = self.transform(mask)\n",
    "        return image, mask, self.image_files[idx]\n",
    "\n",
    "\n",
    "dataset = TempDataset(image_path=\"../original\", mask_path=\"../annotated\")\n",
    "loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "for image, mask, name in loader:\n",
    "    unique = torch.unique(mask)\n",
    "    assert len(unique) == 2\n",
    "    assert unique[0] == 0 and unique[1] == 1\n",
    "\n",
    "    image = image.numpy()\n",
    "    mask = mask.numpy()\n",
    "    for i in range(0, image.shape[0]):\n",
    "        plt.imshow(image[i].transpose(1, 2, 0))\n",
    "        plt.imshow(mask[i].transpose(1, 2, 0), alpha=0.5)\n",
    "        plt.title(name[i])\n",
    "        plt.figure(figsize=(10, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
