{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if it exists\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "\n",
    "img_dir = \"original/\"\n",
    "mask_dir = \"annotated/\"\n",
    "\n",
    "img_paths = [file for file in Path(img_dir).iterdir() if not file.name.startswith(\".\")]\n",
    "mask_paths = [file for file in Path(mask_dir).iterdir() if not file.name.startswith(\".\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path    object\n",
      "mask_paths    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\"image_path\": img_paths,\n",
    "                   \"mask_paths\": mask_paths}, dtype=str)\n",
    "\n",
    "df.head()\n",
    "result=df.dtypes\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "class CuffDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        # df contains the paths to all files\n",
    "        self.df = df\n",
    "        # transforms is the set of data augmentation operations\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):                   \n",
    "        image = cv2.imread(self.df.iloc[idx, 0])\n",
    "        mask = cv2.imread(self.df.iloc[idx, 1], 0)\n",
    "\n",
    "        #image = self.transforms(image)\n",
    "        #mask = self.transforms(mask)\n",
    "        augmented = self.transforms(image=image,\n",
    "                                    mask=mask)\n",
    "\n",
    "        #image = augmented['image'] # Dimension (3, 255, 255)\n",
    "        #mask = augmented['mask']   # Dimension (255, 255)\n",
    "\n",
    "        # We notice that the image has one more dimension (3 color channels), so we have to one one \"artificial\" dimension to the mask to match it\n",
    "        mask = np.expand_dims(mask, axis=0) # Dimension (1, 255, 255)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb Cell 6\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39malbumentations\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mA\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39malbumentations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorch\u001b[39;00m \u001b[39mimport\u001b[39;00m ToTensorV2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m PATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/albumentations/__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m absolute_import\n\u001b[1;32m      3\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1.3.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maugmentations\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserialization\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/albumentations/augmentations/__init__.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Common classes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mblur\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mblur\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcrops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/albumentations/augmentations/blur/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/albumentations/augmentations/blur/functional.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39malbumentations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maugmentations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m convolve\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39malbumentations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maugmentations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m scale\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39malbumentations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maugmentations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     _maybe_process_in_chunks,\n\u001b[1;32m     12\u001b[0m     clipped,\n\u001b[1;32m     13\u001b[0m     preserve_shape,\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/albumentations/augmentations/functional.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mskimage\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39malbumentations\u001b[39;00m \u001b[39mimport\u001b[39;00m random_utils\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39malbumentations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maugmentations\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     MAX_VALUES_BY_DTYPE,\n\u001b[1;32m     13\u001b[0m     _maybe_process_in_chunks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     preserve_shape,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/skimage/__init__.py:122\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# We are not importing the rest of the scikit during the build\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[39m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_shared\u001b[39;00m \u001b[39mimport\u001b[39;00m geometry\n\u001b[1;32m    123\u001b[0m         \u001b[39mdel\u001b[39;00m geometry\n\u001b[1;32m    124\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32mgeometry.pyx:1\u001b[0m, in \u001b[0;36minit skimage._shared.geometry\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "PATCH_SIZE = 256\n",
    "\n",
    "strong_transforms = A.Compose([\n",
    "    A.RandomCrop(width = PATCH_SIZE, height = PATCH_SIZE, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n",
    "\n",
    "    # Pixels\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.RandomGamma(p=0.25),\n",
    "    A.Emboss(p=0.25),  # replaced A.IAAEmboss with A.Emboss\n",
    "    A.Blur(p=0.01, blur_limit = 3),\n",
    "\n",
    "    # Affine\n",
    "    A.OneOf([\n",
    "        A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "        A.GridDistortion(p=0.5),\n",
    "        A.OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)\n",
    "    ], p=0.8),\n",
    "\n",
    "    A.Normalize(p=1.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "transforms = A.Compose([\n",
    "    A.Resize(width = PATCH_SIZE, height = PATCH_SIZE, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Transpose(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n",
    "\n",
    "    ToTensorV2(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Transformations\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "PATCH_SIZE = 256\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Scale the image up to a square of 40 pixels in both height and width\n",
    "    # transforms.Resize(), #  NEed to check pixel size and height LOL\n",
    "    # Randomly crop a square image of 40 pixels in both height and width to\n",
    "    # produce a small square of 0.64 to 1 times the area of the original\n",
    "    # image, and then scale it to a square of 32 pixels in both height and\n",
    "    # width\n",
    "   #  transforms.RandomResizedCrop(, scale=(0.64, 1.0),\n",
    "    #                                                ratio=(1.0, 1.0)),\n",
    "    transforms.RandomCrop(size=(PATCH_SIZE, PATCH_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    # Standardize each channel of the image\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], # Is this the best normalization? Or [0.5, 0.5, 0.5]?\n",
    "     [0.5, 0.5, 0.5])])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], # Is this the best normalization? Or [0.5, 0.5, 0.5]?\n",
    "     [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (28, 2) \n",
      "Val: (8, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split df into train and test data\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {train_df.shape} \\nVal: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 960, 1440] doesn't match the broadcast shape [3, 960, 1440]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m CuffDataset(train_df, transform_test)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m images, masks \u001b[39m=\u001b[39m train_dataset[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(images)\n",
      "\u001b[1;32m/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb Cell 8\u001b[0m line \u001b[0;36mCuffDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m mask \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39miloc[idx, \u001b[39m1\u001b[39m], \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms(image)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms(mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#augmented = self.transforms(image=image,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#                            mask=mask)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# We notice that the image has one more dimension (3 color channels), so we have to one one \"artificial\" dimension to the mask to match it\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(mask, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m# Dimension (1, 255, 255)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:270\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    263\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    362\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39mdiv_(std)\n\u001b[1;32m    364\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 960, 1440] doesn't match the broadcast shape [3, 960, 1440]"
     ]
    }
   ],
   "source": [
    "train_dataset = CuffDataset(train_df, transform_test)\n",
    "images, masks = train_dataset[0]\n",
    "print(images)\n",
    "# print(train_dataset)\n",
    "#mean, std = images.mean([0,2,3]), images.std([0,2,3])\n",
    "#print(mean, std)\n",
    "\n",
    "#image, mask = train_dataset[0]  # get the first item\n",
    "#print(image.shape, mask.shape)\n",
    "#print(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CuffDataset(train_df, transforms=transform_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=28, shuffle=False)\n",
    "\n",
    "val_dataset = CuffDataset(val_df, transforms=transform_test)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mean(): input dtype should be either floating point or complex dtypes. Got Byte instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb Cell 10\u001b[0m line \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef mean_std(loader):\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m  images, labels = next(iter(loader))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m  return mean, std\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m images, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_dataloader))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/karenliu/Documents/GitHub/mobile_health_segmentation/load_data.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m mean, std \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39;49mmean([\u001b[39m0\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m]), images\u001b[39m.\u001b[39mstd([\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mean(): input dtype should be either floating point or complex dtypes. Got Byte instead."
     ]
    }
   ],
   "source": [
    "'''\n",
    "def mean_std(loader):\n",
    "  images, labels = next(iter(loader))\n",
    "  # shape of images = [b,c,w,h]\n",
    "  mean, std = images.mean([0,2,3]), images.std([0,2,3])\n",
    "  return mean, std\n",
    "'''\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "mean, std = images.mean([0,2,3]), images.std([0,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
